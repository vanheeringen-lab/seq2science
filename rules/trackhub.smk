rule bedgraphish_to_bedgraph:
    """
    Convert the bedgraph-ish file generated by genrich into true bedgraph file.
    """
    input:
        expand("{result_dir}/genrich/{{assembly}}-{{sample}}.bdgish", **config)
    output:
        bedgraph=expand("{result_dir}/genrich/{{assembly}}-{{sample}}.bedgraph", **config)
    wildcard_constraints:
        sample=any_sample()
    log:
        expand("{log_dir}/bedgraphish_to_bedgraph/{{assembly}}-{{sample}}.log", **config)
    benchmark:
        expand("{benchmark_dir}/bedgraphish_to_bedgraph/{{assembly}}-{{sample}}.log", **config)[0]
    shell:
        """
        splits=$(grep -Pno "([^\/]*)(?=\.bam)" {input})
        splits=($splits)
        lncnt=$(wc -l {input} | echo "$(grep -Po ".*(?=\ )")":)
        splits+=($lncnt)

        counter=1
        for split in "${{splits[@]::${{#splits[@]}}-1}}";
        do
            filename=$(grep -Po "(?<=:).*" <<< $split);
            if [[ $filename =~ {wildcards.sample} ]]; then
                startnr=$(grep -Po ".*(?=\:)" <<< $split);
                endnr=$(grep -Po ".*(?=\:)" <<< ${{splits[counter]}});

                lines="NR>=startnr && NR<=endnr {{ print \$1, \$2, \$3, \$4 }}"
                lines=${{lines/startnr/$((startnr + 2))}}
                lines=${{lines/endnr/$((endnr - 1))}}

                awk "$lines" {input} > {output}
            fi
            ((counter++))
        done
        """


def find_bedgraph(wildcards):
    if wildcards.peak_caller == 'genrich':
        suffix = '.bedgraph'
    elif wildcards.peak_caller == 'macs2':
        suffix = '_treat_pileup.bdg'
    else:
        suffix = '.bedgraph'

    return f"{config['result_dir']}/{wildcards.peak_caller}/{wildcards.sample}-{wildcards.assembly}{suffix}"


rule bedgraph_bigwig:
    """
    Convert a bedgraph file into a bigwig.
    """
    input:
        bedgraph=find_bedgraph,
        genome_size=expand("{genome_dir}/{{assembly}}/{{assembly}}.fa.sizes", **config)
    output:
        out=expand("{result_dir}/{{peak_caller}}/{{assembly}}-{{sample}}.bw", **config),
        tmp=temp(expand("{result_dir}/{{peak_caller}}/{{assembly}}-{{sample}}.bedgraphtmp", **config))
    wildcard_constraints:
        sample=any_sample()
    log:
        expand("{log_dir}/bedgraph_bigwig/{{peak_caller}}/{{assembly}}-{{sample}}.log", **config)
    benchmark:
        expand("{benchmark_dir}/bedgraphish_to_bedgraph/{{assembly}}-{{sample}}-{{peak_caller}}.log", **config)[0]
    conda:
        "../envs/ucsc.yaml"
    shell:
        """
        awk -v OFS='\\t' '{{print $1, $2, $3, $4}}' {input.bedgraph} | sed '/experimental/d' |
        bedSort /dev/stdin {output.tmp} > {log} 2>&1;
        bedGraphToBigWig {output.tmp} {input.genome_size} {output.out} > {log} 2>&1
        """


rule narrowpeak_bignarrowpeak:
    """
    Convert a narrowpeak file into a bignarrowpeak file.
    """
    input:
        narrowpeak= expand("{result_dir}/{{peak_caller}}/{{assembly}}-{{sample}}_peaks.narrowPeak", **config),
        genome_size=expand("{genome_dir}/{{assembly}}/{{assembly}}.fa.sizes", **config)
    output:
        out=     expand("{result_dir}/{{peak_caller}}/{{assembly}}-{{sample}}.bigNarrowPeak", **config),
        tmp=temp(expand("{result_dir}/{{peak_caller}}/{{assembly}}-{{sample}}.tmp.narrowPeak", **config))
    wildcard_constraints:
        sample=any_sample()
    log:
        expand("{log_dir}/narrowpeak_bignarrowpeak/{{peak_caller}}/{{assembly}}-{{sample}}.log", **config)
    benchmark:
        expand("{benchmark_dir}/bedgraphish_to_bedgraph/{{assembly}}-{{sample}}-{{peak_caller}}.log", **config)[0]
    conda:
        "../envs/ucsc.yaml"
    shell:
        """
        # keep first 10 columns, idr adds extra columns we do not need for our bigpeak
        cut -d$'\t' -f 1-10 {input.narrowpeak} |
        bedSort /dev/stdin {output.tmp} > {log} 2>&1;
        bedToBigBed -type=bed4+6 -as=../../schemas/bigNarrowPeak.as {output.tmp} {input.genome_size} {output.out} > {log} 2>&1
        """


def get_bigfiles(wildcards):
    bigfiles = {}
    bigfiles['bigwigs'] = []; bigfiles['bigpeaks'] = []

    # get all the peak files for all replicates or for the replicates combined
    if 'condition' in samples:
        for condition in set(samples['condition']):
            for assembly in set(samples[samples['condition'] == condition]['assembly']):
                bigfiles['bigpeaks'].extend(expand(f"{{result_dir}}/{{peak_caller}}/{condition}-{assembly}.bigNarrowPeak", **config))
    else:
        for sample in samples.index:
            bigfiles['bigpeaks'].extend(expand(f"{{result_dir}}/{{peak_caller}}/{samples.loc[sample, 'assembly']}-{sample}.bigNarrowPeak", **config))

    # get all the bigwigs
    if config.get('combine_replicates', '') == 'merge' and 'condition' in samples:
        for condition in set(samples['condition']):
            for assembly in set(samples[samples['condition'] == condition]['assembly']):
                bigfiles['bigwigs'].extend(expand(f"{{result_dir}}/{{peak_caller}}/{condition}-{assembly}.bw", **config))
    else:
        for sample in samples.index:
            bigfiles['bigwigs'].extend(expand(f"{{result_dir}}/{{peak_caller}}/{samples.loc[sample, 'assembly']}-{sample}.bw", **config))

    return bigfiles


rule trackhub:
    """
    Generate a trackhub which has to be hosted on your own machine, but can then be viewed through the UCSC genome
    browser.
    """
    input:
        unpack(get_bigfiles)
    output:
        directory(expand("{result_dir}/trackhub/", **config))
    log:
        "log/trackhub.log"
    benchmark:
        expand("{benchmark_dir}/trackhub.log", **config)[0]
    run:
        import os
        import re
        import trackhub
        from contextlib import redirect_stdout

        with open(str(log), 'w') as f:
            with redirect_stdout(f):
                # start a shared hub
                hub = trackhub.Hub(hub=f"{os.path.basename(os.getcwd())} trackhub",
                                   short_label=f"{os.path.basename(os.getcwd())} trackhub",
                                   long_label="Automated trackhub generated by the snakemake-workflows tool: \n"
                                              "https://github.com/vanheeringen-lab/snakemake-workflows",
                                   email=config.get('email', 'none@provided.com'))

                # link the genomes file to the hub
                genomes_file = trackhub.genomes_file.GenomesFile()
                hub.add_genomes_file(genomes_file)

                for assembly in set(samples['assembly']):
                    # TODO: add assembly hub support
                    # now add each assembly to the genomes_file
                    genome = trackhub.Genome(assembly)
                    genomes_file.add_genome(genome)

                    # each trackdb is added to the genome
                    trackdb = trackhub.trackdb.TrackDb()
                    genome.add_trackdb(trackdb)
                    priority = 1

                    for peak_caller in config['peak_caller']:
                        conditions = set()
                        for sample in samples[samples['assembly'] == assembly].index:
                            if 'condition' in samples:
                                if samples.loc[sample, 'condition'] not in conditions:
                                    bigpeak = f"{config['result_dir']}/{peak_caller}/{samples.loc[sample, 'condition']}-{assembly}.bigNarrowPeak"
                                else:
                                    bigpeak = False
                                conditions.add(samples.loc[sample, 'condition'])
                                sample_name = f"{samples.loc[sample, 'condition']}{peak_caller}PEAK"
                            else:
                                bigpeak = f"{config['result_dir']}/{peak_caller}/{assembly}-{sample}.bigNarrowPeak"
                                sample_name = f"{sample}{peak_caller}PEAK"

                            if bigpeak:
                                track = trackhub.Track(
                                    name=sample_name,           # track names can't have any spaces or special chars.
                                    source=bigpeak,             # filename to build this track from
                                    visibility='dense',         # shows the full signal
                                    tracktype='bigNarrowPeak',  # required when making a track
                                    priority=priority
                                )
                                priority += 1
                                trackdb.add_tracks(track)

                            bigwig = f"{config['result_dir']}/{peak_caller}/{assembly}-{sample}.bw"
                            sample_name = f"{sample}{peak_caller}BW"

                            track = trackhub.Track(
                                name=sample_name,    # track names can't have any spaces or special chars.
                                source=bigwig,       # filename to build this track from
                                visibility='full',   # shows the full signal
                                color='0,0,0',       # black
                                autoScale='on',      # allow the track to autoscale
                                tracktype='bigWig',  # required when making a track
                                priority = priority
                            )

                            # each track is added to the trackdb
                            trackdb.add_tracks(track)
                            priority += 1

                # now finish by storing the result
                trackhub.upload.upload_hub(hub=hub, host='localhost', remote_dir=output[0])
