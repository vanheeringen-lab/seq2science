# Per rule explanation

This is an automatically generated list of all supported rules, their docstrings, and command. At the start of each workflow run a list is printed of which rules will be run. And while the workflow is running it prints which rules are being started and finished. This page is here to give an explanation to the user about what each rule does, and for developers to find what is, and isn't yet supported.

#### bam2cram
Convert a bam file to the more compressed cram format.
```
samtools view -T {input.assembly} -C {input.bam} -@ {threads} > {output} 2> {log}
```

#### bam_bigwig
Convert a bam file into a bigwig file.
Can output strand specific bam
```
bamCoverage --bam {input.bam} --outFileName {output} {params.strand_filter} --numberOfProcessors {threads} {params.flags} --verbose >> {log} 2>&1
```

#### bedgraph_bigwig
Convert a bedgraph file into a bigwig.
```
awk -v OFS='\\t' '{{print $1, $2, $3, $4}}' {input.bedgraph} | sed '/experimental/d' |
bedSort /dev/stdin {output.tmp} > {log} 2>&1
bedGraphToBigWig {output.tmp} {input.genome_size} {output.out} >> {log} 2>&1
```

#### bedgraphish_to_bedgraph
Convert the bedgraph-ish file generated by genrich into a true bedgraph file.
```
splits=$(grep -Pno "([^\/]*)(?=\.bam)" {input})
splits=($splits)
lncnt=$(wc -l {input} | echo "$(grep -Po ".*(?=\ )")":)
splits+=($lncnt)

counter=1
for split in "${{splits[@]::${{#splits[@]}}-1}}";
do
    filename=$(grep -Po "(?<=:).*" <<< $split);
    if [[ $filename =~ {wildcards.sample} ]]; then
        startnr=$(grep -Po ".*(?=\:)" <<< $split);
        endnr=$(grep -Po ".*(?=\:)" <<< ${{splits[counter]}});

        lines="NR>=startnr && NR<=endnr {{ print \$1, \$2, \$3, \$4 }}"
        lines=${{lines/startnr/$((startnr + 2))}}
        lines=${{lines/endnr/$((endnr - 1))}}

        awk "$lines" {input} > {output}
    fi
    ((counter++))
done
```

#### bedtools_slop
After combine_peaks we end up with just a bed file of summits. We extend all peaks
to a total width of 200, for a fair comparison between peaks.
```
bedtools slop -i {input.bedfile} -g {input.sizes} -b {params.slop} | uniq > {output} 2> {log}
```

#### blind_clustering
Create a sample distance matrix plot per assembly

#### bowtie2_align
Align reads against a genome (index) with bowtie2, and pipe the output to the required sorter(s).
```
bowtie2 {params.params} --threads {threads} -x {input.index}/{wildcards.assembly} {params.input} 2> {log} | tee {output} 1> /dev/null 2>> {log}
```

#### bowtie2_index
Make a genome index for bowtie2. This index is required for alignment.
```
mkdir -p {output}

bowtie2-build {params} --threads {threads} {input} {output}/{wildcards.assembly} > {log} 2>&1
```

#### bwa_index
Make a genome index for bwa (mem). This index is required for alignment.
```
mkdir -p {output}

bwa index -p {params.prefix} {params.params} {input} > {log} 2>&1
```

#### bwa_mem
Align reads against a genome (index) with bwa-mem, and pipe the output to the required sorter(s).
```
bwa mem {params.params} -t {threads} {params.index_dir} {input.reads} 2> {log} | tee {output} 1> /dev/null 2>> {log}
```

#### bwa_mem2
Align reads against a genome (index) with bwa-mem2, and pipe the output to the required sorter(s).
```
bwa-mem2 mem {params.params} -t {threads} {params.index_dir} {input.reads} 2> {log} | tee {output} 1> /dev/null 2>> {log}
```

#### bwa_mem2_index
Make a genome index for bwa-mem2. This index is required for alignment.
```
mkdir -p {output}

bwa-mem2 index -p {params.prefix} {input} > {log} 2>&1
```

#### call_peak_genrich
Call peaks with genrich based on the pileup.
```
Genrich -P -f {input.log} -o {output.narrowpeak} {params} -v > {log} 2>&1
```

#### chipseeker
Generate a multiqc header file with contact info and date of multiqc generation.

#### combine_peaks
Uses gimmemotifs' combine_peaks to "combine" peaks. This finds all peaks close
together and takes the most significant one as the true peak.
```
combine_peaks -i {input.summitfiles} -g {input.sizes} \
--window {params.windowsize} > {output} 2> {log}
```

#### combine_qc_files
Aggregate all the quality control metrics for every sample into a single 
multiqc report.

The input can get very long (causing problems with the shell), so we have 
to write the input to a file, and then we can use that file as input to 
multiqc.

#### complement_blacklist
Take the complement of the blacklist. We need this complement to tell samtools
to only keep reads that are in the complement of the blacklist.
```
sortBed -faidx {input.sizes} -i {input.blacklist} |
complementBed -i stdin -g {input.sizes} > {output} 2> {log}
```

#### computeMatrix
Pre-compute correlations between bams using deeptools.
```
computeMatrix scale-regions -S {input.bw} {params.labels} -R {params.annotation} \
-p {threads} -b 2000 -a 500 -o {output} > {log} 2>&1
```

#### count_matrix
Combine count tables into one count matrix per assembly

#### count_matrix_DEXseq
Combine DEXSeq counts into one count matrix per assembly
for use in function `DEXSeqDataSet()`

#### coverage_table
Use gimmemotif's coverage_table to generate a count table with the nr of reads
under each peak per sample.
```
echo "# The number of reads under each peak" > {output} 
coverage_table -p {input.peaks} -d {input.replicates} 2> {log} | grep -vE "^#" 2>> {log} |  
awk 'BEGIN {{ FS = "@" }} NR==1{{gsub("{wildcards.assembly}-|.samtools-coordinate","",$0)}}; \
{{print $0}}' >> {output}
```

#### create_SNAP_object
Create a snapobject for each BAM file. 

These snapobjects can be merged later using snaptools in R.
```
snaptools snap-pre --input-file={input.bams} --output-snap={output} --genome-name={wildcards.assembly} \
--genome-size={input.genome_size} {params.params} {params.chrm} {params.mapq} > {log} 2>&1
```

#### create_bins_SNAP_object
Add a Binned genome matrix to the SNAPobject, after which it is renamed and moved
to the Snapfiles folder for downstream analysis in R using Snaptools
```
snaptools snap-add-bmat --snap-file={input} {params} > {log} 2>&1
echo "bmat added, moving file" >> {log} 2>&1
mv {input} {output}    >> {log} 2>&1
```

#### cytoband
Generate a cytoband track for each assembly

source: http://genomewiki.ucsc.edu/index.php/Assembly_Hubs#Cytoband_Track
```
cat {input.sizes} | 
bedSort /dev/stdin /dev/stdout | 
awk '{{print $1,0,$2,$1,"gneg"}}' > {output.cytoband_bd}

bedToBigBed -type=bed4 {output.cytoband_bd} -as={params.schema} \
{input.sizes} {output.cytoband_bb} >> {log} 2>&1
```

#### decoy_transcripts
Generate decoy_transcripts.txt for Salmon indexing  

script source: https://github.com/COMBINE-lab/SalmonTools
```
Generate a decoy aware transcriptome index for Salmon.
```

#### deseq2
Differential gene expression analysis with DESeq2.

#### dexseq_count
count exon usage
```
current_conda_env=$(conda env list | grep \* | cut -d "*" -f2-)
DEXseq_path=${{current_conda_env}}/lib/R/library/DEXSeq/python_scripts

python ${{DEXseq_path}}/dexseq_count.py -f bam -r pos {params.endedness} -s {params.strandedness} {input.gff} {input.bam} {output} > {log} 2>&1
```

#### dupRadar
visualize fraction of artifactual reads to normal read duplication
(due to natural over-sequencing of highly expressed genes).

#### dupRadar_combine
Combine the individual images (so we can group them nicely in the MultiQC).
```
convert {params.good_example} {params.bad_example} {input} -append {output} 2> {log}
```

#### edgeR_normalization
edgeR supports three different types of normalization: TMM, RLE, and upperquartile.

TMM: is the weighted trimmed mean of M-values proposed by Robinson and Oshlack (2010).
RLE: is the scaling factor method proposed by Anders and Huber (2010). DEseq2 normalisation
 is based on this.
upperquartile: is the upper-quartile normalization method of Bullard et al (2010).

In addition to 

#### ena2fastq_PE
Download paired-end fastq files directly from the ENA.

#### ena2fastq_SE
Download single-end fastq files directly from the ENA.

#### extend_genome
Append given file(s) to genome
```
# extend the genome.fa
cp {input.genome} {output.genome}

for FILE in {input.extension}; do
    cat $FILE >> {output.genome}
done
```

#### extend_genome_annotation
Append given file(s) to genome annotation
```
# extend the genome.annotation.gtf
cp {input.gtf} {output.gtf}

for FILE in {input.extension}; do
    cat $FILE >> {output.gtf}
done

# generate an extended genome.annotation.bed
gtfToGenePred {output.gtf} {output.gp}
genePredToBed {output.gp} {output.bed}
```

#### fastp_PE
Automated adapter detection, adapter trimming, and quality trimming through fastp (paired-end).
```
fastp -w {params.threads} --in1 {input[0]} --in2 {input[1]} \
--out1 {output.r1} --out2 {output.r2} -h {output.qc_html} -j {output.qc_json} \
{params.config} > {log} 2>&1
```

#### fastp_SE
Automated adapter detection, adapter trimming, and quality trimming through fastp (single-end).
```
fastp -w {params.threads} --in1 {input} --out1 {output.se} -h {output.qc_html} -j {output.qc_json} \
{params.config} > {log} 2>&1
```

#### fastp_qc_PE
Get quality scores for (technical) replicates
```
fastp -w {threads} --in1 {input[0]} --in2 {input[1]} \
-h {output.qc_html} -j {output.qc_json} \
{params.config} > {log} 2>&1
```

#### fastp_qc_SE
Get quality scores for (technical) replicates
```
fastp -w {threads} --in1 {input} -h {output.qc_html} -j {output.qc_json} > {log} 2>&1
```

#### fastq_pair
fastq_pair re-writes paired-end fastq files to ensure that each read has a mate and 
dsicards singleton reads. This step is required after scRNA trimming since we only trim the fastq 
containing reads and not the barcode fastq. 
```
gunzip -c {input.r1} > {output.intermediates1[0]} 2> {log}
gunzip -c {input.r2} > {output.intermediates1[1]} 2>> {log}
if [ {params.tused} == true ]
then
  opts="{params.options}"
else
  echo "\nsetting parameter t with the number of reads in the fastq\n" >> {log}
  opts="-p -t "$(wc -l {input.r1} | grep -Po '^\d+' | awk '{{print int($1/4)}}')
fi
fastq_pair $opts {output.intermediates1} >> {log} 2>&1
```

#### fastqc
Generate quality control report for fastq files.
```
fastqc {input} -O {params} > {log} 2>&1
```

#### featureCounts
Use featureCounts to generate the fraction reads in peaks score 
(frips/assigned reads). See: https://www.biostars.org/p/337872/ and
https://www.biostars.org/p/228636/
```
# Make a custom "SAF" file which featureCounts needs:
awk 'BEGIN{{FS=OFS="\t"; print "GeneID\tChr\tStart\tEnd\tStrand"}}{{print $4, $1, $2+1, $3, "."}}' \
{input.peak} 1> {output.tmp_saf} 2> {log}

# run featureCounts
featureCounts -T {threads} -p -a {output.tmp_saf} -F SAF -o {output.real_out} {input.bam} > {log} 2>&1

# move the summary to qc directory
mv $(dirname {output.real_out})/$(basename {output.summary}) {output.summary}
```

#### featurecounts
summarize reads to gene level. Outputs a counts table per bam file.
```
featureCounts -a {input.gtf} {input.bam} {params.endedness} -s {params.strandedness} {params.user_flags} -T {threads} -o {output} > {log} 2>&1
```

#### gcPercent
Generate a gc content track

source: http://hgdownload.cse.ucsc.edu/goldenPath/hg19/gc5Base/
```
        hgGcPercent -wigOut -doGaps -file=stdout -win=5 -verbose=0 {wildcards.assembly} {input.twobit} \
| gzip > {output.gcpcvar}

        wigToBigWig {output.gcpcvar} {input.sizes} {output.gcpc} >> {log} 2>&1
        
```

#### gene_id2name
Parse the gtf file to generate a gene_id to gene_name conversion table.

#### genrich_pileup
Generate the pileup. We do this separately from peak-calling since these two
processes have a very different computational footprint.
```
input=$(echo {input.reps} | tr ' ' ',')
Genrich -X -t $input -f {output.log} {params.control} -k {output.bedgraphish} \
{params.params} -v > {log} 2>&1
```

#### get_genome
Download a genome through genomepy.

Also download a blacklist if it exists.

#### get_genome_annotation
Download a gene annotation through genomepy.

#### get_genome_support_files
Generate supporting files for a genome.

#### get_transcripts
Generate transcripts.fasta using gffread.

Requires genome.fa and annotation.gtf (with matching chromosome/scaffold names)

#### hisat2_align
Align reads against a genome (index) with hisat2, and pipe the output to the required sorter(s).
```
hisat2 {params.params} --threads {threads} -x {input.index}/part {params.input} 2> {log} | tee {output} 1> /dev/null 2>> {log}
```

#### hisat2_index
Make a genome index for hisat2. This index is required for alignment.
```
mkdir -p {output}

hisat2-build {params} -p {threads} {input} {output}/part > {log} 2>&1
```

#### hisat2_splice_aware_index
Make an exon-junction and splice aware index for hisat2. 
This index is required for alignment and quantification of RNA-seq data.
```
mkdir -p {output}

hp=$(which hisat2)
python3 ${{hp}}_extract_splice_sites.py {input.gtf} > {output}/splice_sites.tsv 2>> {log}
python3 ${{hp}}_extract_exons.py {input.gtf} > {output}/exons.tsv 2>> {log}

hisat2-build {params} -p {threads} --ss {output}/splice_sites.tsv --exon {output}/exons.tsv \
{input.fasta} {output}/part >> {log} 2>&1
```

#### hmmratac
Call gappedpeaks with HMMRATAC.
```
HMMRATAC --bedgraph true -o {params.basename} {params.hmmratac_params} -Xmx22G -b {input.bam} -i {input.bam_index} -g {input.genome_size} > {log} 2>&1
```

#### hmmratac_genome_info
Generate the 'genome info' that hmmratac requires for peak calling.
https://github.com/LiuLabUB/HMMRATAC/issues/17

TODO: isnt this just .fa.sizes?
```
samtools view -H {input} 2>  {log} | grep SQ 2>> {log} | cut -f 2-3 2>> {log} | cut -d ':' -f 2   2>> {log} | cut -f 1        > {output.tmp1} 2> {log}
samtools view -H {input} 2>> {log} | grep SQ 2>> {log} | cut -f 2-3 2>> {log} | cut -d ':' -f 2,3 2>> {log} | cut -d ':' -f 2 > {output.tmp2} 2> {log}
paste {output.tmp1} {output.tmp2} > {output.out} 2> {log}
```

#### htseq_count
summarize reads to gene level. Outputs a counts table per bam file.
```
htseq-count {input.bam} {input.gtf} -r pos -s {params.strandedness} {params.user_flags} -n {threads} -c {output} > {log} 2>&1
```

#### idr
Combine replicates based on the irreproducible discovery rate (IDR). Can only handle two replicates.
For more than two replicates use fisher's method.

When combining narrowpeak files with IDR, the q-score and summit are set to -1 (means not set). However some
downstream tools require these to be set. So we set the q-score to zero, and place the summit of the peak in
the middle of the peak.
```
                if [ "{params.nr_reps}" == "1" ]; then
cp {input} {output.true}
touch {output.temp}
                else
idr --samples {input} {params.rank} --output-file {output.temp} > {log} 2>&1
if [ "{wildcards.ftype}" == "narrowPeak" ]; then
    awk 'IFS="\t",OFS="\t" {{$9=0; $10=int(($3 - $2) / 2); print}}' {output.temp} > {output.true} 2>> {log}
else
    cp {output.temp} {output.true}
fi
                fi
                
```

#### infer_strandedness
use RSeqQC's infer_experiment.py to determine strandedness af a sample
```
infer_experiment.py -i {input.bam} -r {input.bed} -q {params} 2> {log} | awk NF > {output} 
```

#### insert_size_metrics
Get insert size metrics from a (paired-end) bam. This score is then used by
MultiQC in the report.
```
picard CollectInsertSizeMetrics INPUT={input} \
OUTPUT={output.tsv} H={output.pdf} > {log} 2>&1
```

#### kallistobus_count
Align reads against a transcriptome (index) with kallistobus and output a quantification file per sample.
```
kb count \
-i {params.basename}.idx -w {input.barcodefile} \
-t {threads} -g {params.basename}_t2g.txt \
-o {output} -c1 {params.basename}_cdna_t2c.txt -c2 {params.basename}_intron_t2c.txt \
{params.options} {input.reads} > {log} 2>&1
```

#### kallistobus_ref
Make a genome index for kallistobus. This index is required for counting.
```
kb ref \
{input.fa} {input.gtf} \
-i {params.basename}.idx -g {params.basename}_t2g.txt -f1 {params.basename}_cdna.fa \
-f2 {params.basename}_intron.fa \
-c1 {params.basename}_cdna_t2c.txt -c2 {params.basename}_intron_t2c.txt \
{params.options} > {log} 2>&1
```

#### keep_mates
In-house script that, after alignment, removes the information that reads are paired.
This can be beneficial when peak calling with macs2 when shifting + extending, since
macs2 in this case only keeps the first in pair.

#### linked_txome
Generate a linked transcriptome for tximeta

Also creates a symlink to the gtf in an Ensembl format (required by tximeta)

Required to converting salmon output (estimated transcript abundances) to gene counts

#### log_normalization
Log1p normalization of a count table.

#### macs2_callpeak
Call peaks using macs2.
Macs2 requires a genome size, which we estimate from the amount of unique kmers of the average read length.
```
echo "preparing to run unique-kmers.py with -k $kmer_size" >> {log}
GENSIZE=$(unique-kmers.py {params.genome} -k $kmer_size --quiet 2>&1 | grep -P -o '(?<=\.fa: ).*')
echo "kmer size: $kmer_size, and effective genome size: $GENSIZE" >> {log}

# call peaks
macs2 callpeak --bdg -t {input.bam} {params.control} --outdir {config[result_dir]}/macs2/ -n {wildcards.assembly}-{wildcards.sample} \
{params.macs_params} -g $GENSIZE -f {params.format} >> {log} 2>&1
```

#### macs_bdgcmp
Prepare p-value files for rule macs_cmbreps
```
macs2 bdgcmp -t {input.treatment} -c {input.control} -m qpois -o {output} > {log} 2>&1
```

#### macs_cmbreps
Combine biological replicates through Fisher's method

(Link original peakfile in replicate_processed if there is only 1 sample for a condition)
```
                    if [ "{params.nr_reps}" == "1" ]; then
touch {output.tmpbdg} {output.tmppeaks}
mkdir -p $(dirname {output.peaks}); cp {input.treatment} {output.peaks}
                    else
macs2 cmbreps -i {input.bdgcmp} -o {output.tmpbdg} -m fisher > {log} 2>&1
macs2 {params.function} {params.config} -i {output.tmpbdg} -o {output.tmppeaks} >> {log} 2>&1
cat {output.tmppeaks} | tail -n +2 > {output.peaks}
                    fi
                    
```

#### mark_duplicates
Mark all duplicate reads in a bam file with picard MarkDuplicates
```
# use the TMPDIR if set, and not given in the config
if [[ ${{TMPDIR:=F}} == "F" ]] || [[ "{params}" == *TMP_DIR* ]]
then
    tmpdir=""
else 
    tmpdir=TMP_DIR=$TMPDIR
fi

picard MarkDuplicates $tmpdir {params} INPUT={input} \
OUTPUT={output.bam} METRICS_FILE={output.metrics} > {log} 2>&1
```

#### mean_center
Mean centering of a count table.

#### merge_replicates
Merge replicates (fastqs) simply by concatenating the files. We also change the name of the read headers to 
contain the name of the original replicate.

Must happen after trimming due to trim-galore's automatic adapter trimming method 

If a replicate has only 1 sample in it, simply move the file.

#### minimap2_align
Align reads against a genome (index) with minimap2, and pipe the output to the required sorter(s).
```
minimap2 -t {threads} -a {input.index} {input.reads} {params} > {output} 2> {log}
```

#### minimap2_index
Make a genome index for minimap2. This index is required for alignment.
```
mkdir -p $(dirname {output})

minimap2 -t {threads} -d {output} {input} {params} > {log} 2>&1
```

#### mt_nuc_ratio_calculator
Estimate the amount of nuclear and mitochondrial reads in a sample. This metric
is especially important in ATAC-seq experiments where mitochondrial DNA can
be overrepresented. Reads are classified as mitochondrial reads if they map
against either "chrM" or "MT".

These values are aggregated and displayed in the MultiQC report.
```
mtnucratio {input.bam} {params.mitochondria}
```

#### multiBamSummary
Pre-compute a bam summary with deeptools.
```
multiBamSummary bins --bamfiles {input.bams} -out {output} {params.names} \
{params.params} -p {threads} > {log} 2>&1
```

#### multiqc_filter_buttons
Generate filter buttons.

#### multiqc_rename_buttons
Generate rename buttons for the multiqc report.

#### multiqc_samplesconfig
Add a section in the multiqc report that reports the samples.tsv and config.yaml

#### multiqc_schema
Generate a multiqc config schema. Used for the ordering of modules.

#### narrowpeak_summit
Convert a narrowpeak file to a "macs2 summits" file.
```
awk 'BEGIN {{OFS="\t"}} {{ print $1,$2+$10,$2+$10+1,$4,$9; }}' {input} > {output} 2> {log}
```

#### onehot_peaks
Get onehot encodings of which peaks are found in which samples
```
        awk '{{print $1":"$2"-"$3}}' {input.combinedpeaks} > {output.real} 2> {log}

        for brep in {input.narrowpeaks}
        do
            bedtools sort -i $brep 2> {log} |
            bedtools intersect -a {input.combinedpeaks} -b stdin -c 2> {log} |
            awk '{{print $4}}' 2> {log} |
            paste {output.real} - > {output.tmp}
            mv {output.tmp} {output.real}
        done
        
        echo -e "# onehot encoding of which condition contains which peaks\\n$(cat {output.real})" > {output.tmp} && cp {output.tmp} {output.real}
        
```

#### peak_bigpeak
Convert a narrowpeak file into a bignarrowpeak file.
https://genome-source.gi.ucsc.edu/gitlist/kent.git/tree/master/src/hg/lib/
```
# keep first 10 columns, idr adds extra columns we do not need for our bigpeak
cut -d$'\t' -f 1-{params.columns} {input.narrowpeak} |
awk -v OFS="\t" '{{$5=$5>1000?1000:$5}} {{print}}' | 
bedSort /dev/stdin {output.tmp} > {log} 2>&1

bedToBigBed -type={params.type} -as={params.schema} {output.tmp} \
{input.genome_size} {output.out} >> {log} 2>&1
```

#### plotCorrelation
Calculate the correlation between bams with deeptools.
```
plotCorrelation --corData {input} --plotFile {output} -c {wildcards.method} \
-p heatmap --plotTitle {params.title} {params.params} > {log} 2>&1
```

#### plotFingerprint
Plot the "fingerprint" of your bams, using deeptools. 
```
plotFingerprint -b {input.bams} {params} --outRawCounts {output} -p {threads} > {log} 2>&1 
```

#### plotPCA
Plot a PCA between bams using deeptools.
```
plotPCA --corData {input} --outFileNameData {output} > {log} 2>&1
```

#### plotProfile
Plot the so-called profile using deeptools.
```
plotProfile -m {input} --outFileName {output.img} --outFileNameData {output.file} > {log} 2>&1
```

#### prepare_DEXseq_annotation
generate a DEXseq annotation.gff from the annotation.gtf
```
current_conda_env=$(conda env list | grep \* | cut -d "*" -f2-)
DEXseq_path=${{current_conda_env}}/lib/R/library/DEXSeq/python_scripts

python ${{DEXseq_path}}/dexseq_prepare_annotation.py {input} {output} > {log} 2>&1
```

#### quantile_normalization
Quantile normalization is a type of normalization that makes the distribution 
between samples identical. This means that the actual count distribution 
within a sample changes. After quantile normalization, samples are CPM 
normalized (which has no effect between and within samples), for a fair
comparison between normalisation techniques.

See wikipedia:
https://en.wikipedia.org/wiki/Quantile_normalization

#### renamefastq_PE
Create symlinks to fastqs with incorrect fqexts (default R1/R2).
Forward and reverse samples will be switched if forward/reverse names are not 
lexicographically ordered.
```
ln {input[0]} {output[0]}
ln {input[1]} {output[1]}
```

#### run2sra
Download the SRA of a sample by its unique identifier.

Tries first downloading with the faster ascp protocol, if that fails it
falls back on the slower http protocol.
```
# move to output dir since somehow prefetch sometimes puts files in the cwd...
# and remove the top level folder since prefetch will assume we are done otherwise
mkdir -p {params.outdir}; cd {params.outdir}; rm -r {wildcards.run}

# three attempts
for i in {{1..3}}
do
    # acquire a lock
    (
        flock --timeout 30 200 || continue
        sleep 2
    ) 200>{pysradb_cache_lock}

    # dump
    prefetch --max-size 999999999999 --output-directory ./ --log-level debug --progress {wildcards.run} >> {log} 2>&1 && break
    sleep 10
done
```

#### runs2sample
Concatenate a single run or multiple runs together into a fastq

#### salmon_decoy_aware_index
Generate a decoy aware transcriptome index for Salmon.

#### salmon_index
Generate a transcriptome index for Salmon.
```
mkdir -p {output}

salmon index -t {input} -i {output} {params} --threads {threads} > {log} 2>&1
```

#### salmon_quant
Align reads against a transcriptome (index) with Salmon (mapping-based mode) and output a quantification file per sample.
```
salmon quant -i {input.index} -l A {params.input} {params.params} -o {output.dir} \
--threads {threads} > {log} 2>&1
```

#### sam2bam
Convert a file in sam format into a bam format (binary)
```
samtools view -b {input} > {output}
```

#### sambamba_sort
Sort the result of alignment or sieving with the sambamba sorter.
```
sambamba view --nthreads {threads} -f bam  {input} -o /dev/stdout  2> {log} |
sambamba sort --nthreads {threads} {params} /dev/stdin -o {output}  2> {log}
```

#### samtools_index
Create an index of a bam file which can be used for e.g. visualization.
```
samtools index {params} {input} {output}
```

#### samtools_index_cram
Generate the index for a cram file.
```
samtools index {input} {output} > {log} 2>&1
```

#### samtools_presort
(Pre)sort the result of alignment with the samtools sorter.
```
# we set this trap to remove temp files when prematurely ending the rule
trap "rm -f {params.out_dir}/{wildcards.assembly}-{wildcards.sample}.tmp*bam" INT;
rm -f {params.out_dir}/{wildcards.assembly}-{wildcards.sample}.tmp*bam 2> {log}

samtools sort -@ {threads} {params.memory} {input} -o {output} \
-T {params.out_dir}/{wildcards.assembly}-{wildcards.sample}.tmp 2> {log}
```

#### samtools_sort
Sort the result of sieving with the samtools sorter.
```
# we set this trap to remove temp files when prematurely ending the rule
trap "rm -f {params.out_dir}/{wildcards.assembly}-{wildcards.sample}.tmp*bam" INT;
rm -f {params.out_dir}/{wildcards.assembly}-{wildcards.sample}.tmp*bam 2> {log}

samtools sort {params.sort_order} -@ {threads} {params.memory} {input} -o {output} \
-T {params.out_dir}/{wildcards.assembly}-{wildcards.sample}.tmp 2> {log}
```

#### samtools_stats
Get general stats from bam files like percentage mapped.

#### setup_blacklist
Combine the encode blacklist with mitochrondrial dna depending on config.

#### sieve_bam
"Sieve" a bam.

This rule does (at least) one of these:
* filtering on minimum mapping quality
* tn5 shift adjustment
* remove multimappers
* remove reads inside the blacklist
* filter paired-end reads on transcript length
```
samtools view -h {params.prim_align} {params.minqual} {params.blacklist} \
{input.bam} | {params.atacshift} {params.sizesieve}
samtools view -b > {output.final} 2> {log}
```

#### softmask_track_1
Generate a track of all softmasked regions

source: https://github.com/Gaius-Augustus/MakeHub/blob/master/make_hub.py

#### softmask_track_2
Generate a track of all softmasked regions

source: https://github.com/Gaius-Augustus/MakeHub/blob/master/make_hub.py
```
        bedSort {input.mask_unsorted} {output.maskbed} >> {log} 2>&1

        bedToBigBed -type=bed3 {output.maskbed} {input.sizes} {output.mask} >> {log} 2>&1
        
```

#### sra2fastq_PE
Downloaded (raw) SRAs are converted to paired-end fastq files.
Forward and reverse samples will be switched if forward/reverse names are not lexicographically ordered.
```
# move to output dir since somehow parallel-fastq-dump sometimes puts files in the cwd...
mkdir -p {output.tmpdir}; cd {output.tmpdir}

# acquire the lock
(
    flock --timeout 30 200 || exit 1
    sleep 3
) 200>{pysradb_cache_lock}

# dump to tmp dir
parallel-fastq-dump -s {input} -O {output.tmpdir} \
--threads {threads} --split-e --skip-technical --dumpbase \
--readids --clip --read-filter pass --defline-seq '@$ac.$si.$sg/$ri' \
--defline-qual '+' --gzip >> {log} 2>&1

# rename file and move to output dir
mv {output.tmpdir}/*_1* {output.fastq[0]}
mv {output.tmpdir}/*_2* {output.fastq[1]}
```

#### sra2fastq_SE
Downloaded (raw) SRAs are converted to single-end fastq files.
```
# move to output dir since somehow parallel-fastq-dump sometimes puts files in the cwd...
mkdir -p {output.tmpdir}; cd {output.tmpdir}

# acquire the lock
(
    flock --timeout 30 200 || exit 1
    sleep 3
) 200>{pysradb_cache_lock}

# dump to tmp dir
parallel-fastq-dump -s {input} -O {output.tmpdir} \
--threads {threads} --split-spot --skip-technical --dumpbase --readids \
--clip --read-filter pass --defline-seq '@$ac.$si.$sg/$ri' \
--defline-qual '+' --gzip >> {log} 2>&1

# rename file and move to output dir
mv {output.tmpdir}/*.fastq.gz {output.fastq}
```

#### star_align
Align reads against a genome (index) with STAR, and pipe the output to the required sorter(s).
```
trap "find {log} -type f ! -name Log* -exec rm {{}} \;" EXIT
mkdir -p {log}
mkdir -p {output.dir}                

STAR --genomeDir {input.index} --readFilesIn {params.input} --readFilesCommand gunzip -c \
--outSAMtype BAM Unsorted --outStd BAM_Unsorted \
--outFileNamePrefix {log}/ --outTmpDir {output.dir}/STARtmp \
--runThreadN {threads} {params.params} > {output.pipe} 2> {log}/Log.stderr.out

# move all non-log files to output directory (this way the log files are kept on error)
find {log} -type f ! -name Log* -exec mv {{}} {output.dir} \;
```

#### star_index
Make a genome index for STAR.

Troubleshooting:
1) sufficient RAM & disk space?
2) increase the RAM available (--limitGenomeGenerateRAM)
3) reduce the number of threads (seq2science -j 5)
4) reduce accuracy (--genomeSAsparseD 2)

In your config.yaml:
aligner:
star:
index: --limitGenomeGenerateRAM 60000000000 --genomeSAsparseD 1
```
            function log2 {{
local x=0
for (( y=$1-1 ; $y > 0; y >>= 1 )) ; do
    let x=$x+1
done
echo $x
            }}

            # set genome dependent variables
            NBits=""
            NBases=""
            GenomeLength=$(awk -F"\t" '{{x+=$2}}END{{printf "%i", x}}' {input.sizefile})
            NumberOfReferences=$(awk 'END{{print NR}}' {input.sizefile})
            if [ $NumberOfReferences -gt 5000 ]; then
                # for large genomes, --genomeChrBinNbits should be scaled to min(18,log2[max(GenomeLength/NumberOfReferences,ReadLength)])
                # ReadLength is skipped here, as it is unknown
                LpR=$(log2 $((GenomeLength / NumberOfReferences)))
                NBits="--genomeChrBinNbits $(($LpR<18 ? $LpR : 18))"
                printf "NBits: $NBits\n\n" >> {log} 2>&1
            fi

            if [ $GenomeLength -lt 268435456 ]; then
                # for small genomes, --genomeSAindexNbases must be scaled down to min(14, log2(GenomeLength)/2-1)
                logG=$(( $(log2 $GenomeLength) / 2 - 1 ))
                NBases="--genomeSAindexNbases $(( $logG<14 ? $logG : 14 ))"
                printf "NBases: $NBases\n\n" >> {log} 2>&1
            fi

            mkdir -p {output}

            STAR --runMode genomeGenerate --genomeFastaFiles {input.genome} --sjdbGTFfile {input.gtf} \
            --genomeDir {output} --outFileNamePrefix {output}/ \
            --runThreadN {threads} $NBits $NBases {params} >> {log} 2>&1
            
```

#### trackhub
Generate a UCSC track hub/assembly hub. 

To view the hub, the output directory must be hosted on an web accessible location, 
and the location of the hub.txt file given the UCSC genome browser at 
My Data > Track Hubs > My Hubs

#### trackhub_index
Generate a searchable annotation & index for each assembly

sources: https://genome.ucsc.edu/goldenPath/help/hubQuickStartSearch.html
 https://www.biostars.org/p/272649/
```
# generate annotation files
gtfToGenePred -allErrors -ignoreGroupsWithoutExons -geneNameAsName2 -genePredExt {input.gtf} {output.genePred} -infoOut={output.info} >> {log} 2>&1

# check if gtf has gene_names > if l!=0, use gene_name, else transcript_id
l=$(head -n 100 {input.gtf} | (grep gene_name || true) | wc -l)

# switch columns 1 (transcript_id) and 12 (gene_name)
awk -v len=$l 'BEGIN {{ FS = "\t" }}; {{ if(len!="0") {{ t = $1; $1 = $12; $12 = t; print; }} else {{ print; }} }}' {output.genePred} > {output.genePrednamed}

# remove lines with missing headers        
grep -v "^ " {output.genePrednamed} > {output.genePred}

genePredToBed {output.genePred} {output.genePredbed} >> {log} 2>&1

bedSort {output.genePredbed} {output.genePredbed} >> {log} 2>&1

bedToBigBed -extraIndex=name {output.genePredbed} {input.sizes} {output.genePredbigbed} >> {log} 2>&1

# generate searchable indexes (by 1: transcriptID, 2: geneId, 8: proteinID, 9: geneName, 10: transcriptName)
grep -v "^#" {output.info} | awk -v len=$l 'BEGIN {{ FS = "\t" }} ; {{ if(len!="0") {{print $9, $1, $2, $8, $9, $10}} else {{print $1, $1, $2, $8, $9, $10}} }}' > {output.indexinfo}

ixIxx {output.indexinfo} {output.ix} {output.ixx} >> {log} 2>&1
```

#### trimgalore_PE
Automated adapter detection, adapter trimming, and quality trimming through trim galore (paired-end).
```
trim_galore --paired -j {threads} {params.config} -o $(dirname {output.r1}) {input.r1} {input.r2} > {log} 2>&1
    
# now rename to proper output
for f in $(find "$(dirname {output.r1})/" -name "{wildcards.sample}_*val_*.fq.gz"); do
    mv "$f" "$(echo "$f" | sed s/.fq/.{params.fqsuffix}/)"; done
for f in $(find "$(dirname {output.r1})/" -maxdepth 1 -name "{wildcards.sample}_*val_*.{params.fqsuffix}.gz"); do
    mv "$f" "$(echo "$f" | sed s/_val_./_trimmed/)"; done
    
# move the trimming reports to qc directory
for f in $(find "$(dirname {output.r1})/" -name "{wildcards.sample}_*.{params.fqsuffix}.gz_trimming_report.txt"); do
    mv "$f" "$(dirname {output.qc[0]})/$(basename $f)"; done
```

#### trimgalore_SE
Automated adapter detection, adapter trimming, and quality trimming through trim galore (single-end).
```
trim_galore -j {threads} {params.config} -o $(dirname {output.se}) {input} > {log} 2>&1
    
# now rename to proper output
if [ "{params.fqsuffix}" != "fq" ]; then
  mv "$(dirname {output.se})/{wildcards.sample}_trimmed.fq.gz" {output.se}
fi 
    
# move the trimming report to qc directory
report=$(dirname {output.se})/{wildcards.sample}.{params.fqsuffix}.gz_trimming_report.txt
mv $report {output.qc}
```

#### twobit
Generate a 2bit file for each assembly

#### unzip_annotation
Unzip (b)gzipped files.

